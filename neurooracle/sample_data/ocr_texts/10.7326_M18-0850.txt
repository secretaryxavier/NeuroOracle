This is a repository copy of PRISMA Extension for Scoping Reviews (PRISMA-
ScR):Checklist and Explanation.
White Rose Research Online URL for this paper:
https://eprints.whiterose.ac.uk/136633/
Version: Accepted Version
Article:
Tricco, Andrea C, Lillie, Erin, Zarin, Wasifa et al. (25 more authors) (2018) PRISMA 
Extension for Scoping Reviews (PRISMA-ScR):Checklist and Explanation. Annals of 
Internal Medicine. pp. 467-473. ISSN 0003-4819 
https://doi.org/10.7326/M18-0850
eprints@whiterose.ac.uk
https://eprints.whiterose.ac.uk/
Reuse 
Items deposited in White Rose Research Online are protected by copyright, with all rights reserved unless 
indicated otherwise. They may be downloaded and/or printed for private study, or other acts as permitted by 
national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of 
the full text version. This is indicated by the licence information on the White Rose Research Online record 
for the item. 
Takedown 
If you consider content in White Rose Research Online to be in breach of UK law, please notify us by 
emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request.

1 
 
PRISMA Extension for Scoping Reviews (PRISMA-ScR): Checklist and 
1 
Explanation 
2 
Andrea C Triccoa,b*  
 
 
 
Email: triccoa@smh.ca 
3 
Erin Lillieb 
 
 
 
 
 
Email: lilliee@smh.ca 
4 
Wasifa Zarinb  
 
 
 
 
Email: zarinw@smh.ca 
5 
Kelly K O’Brienc,d,e  
 
 
 
Email: kelly.obrien@utoronto.ca 
6 
Heather Colquhounf  
 
 
 
Email: heather.colquhoun@utoronto.ca  
7 
Danielle Levacg 
 
 
 
 
Email: d.levac@northeastern.edu 
8 
David Moherh 
 
 
 
 
Email: dmoher@ohri.ca 
9 
Micah D J Petersi,j  
 
 
 
Email: micah.peters@unisa.edu.au 
10 
Tanya Horsleyk 
 
 
 
 
Email: thorsley@rcpsc.edu 
11 
Laura Weeksl 
 
 
 
 
Email: lauraw@cadth.ca 
12 
Susanne Hempelm  
 
 
 
Email: susanne_hempel@rand.org 
13 
Elie A Akln 
 
 
 
 
 
Email: ea32@aub.edu.lb 
14 
Christine Chango 
 
 
 
 
Email: christine.chang@ahrq.hhs.gov 
15 
Jessie McGowanp  
 
 
 
Email: jmcgowan@uottawa.ca 
16 
Lesley Stewartq 
 
 
 
 
Email: lesley.stewart@york.ac.uk 
17 
Lisa Hartlingr  
 
 
 
 
Email: hartling@ualberta.ca 
18 
Adrian Aldcrofts 
 
 
 
 
Email: aaldcroft@bmj.com 
19 
Michael G Wilsont  
 
 
 
Email: wilsom2@mcmaster.ca 
20 
Chantelle Garrittyh  
 
 
 
Email: cgarritty@ohri.ca 
21 
Simon Lewinu,v 
 
 
 
 
Email: simon.lewin@fhi.no 
22 
Christina M Godfreyw 
 
 
 
Email: godfreyc@queensu.ca 
23

2 
 
Marilyn T Macdonaldx 
 
 
 
Email: marilyn.macdonald@dal.ca 
24 
Etienne V Langloisy  
 
 
 
Email: langloise@who.int 
25 
Karla Soares-Weiserz 
 
 
 
Email: ksoares-weiser@cochrane.org 
26 
Jo Moriartyaa  
 
 
 
 
Email: jo.moriarty@kcl.ac.uk  
27 
Tammy Cliffordl 
 
 
 
 
Email: tammyc@cadth.ca 
28 
Özge Tunçalpab,ac 
 
 
 
 
Email: tuncalpo@who.int 
29 
Sharon E Strausa,ad   
 
 
 
Email: sharon.straus@utoronto.ca 
30 
 
31 
Author Affiliations 
32 
aKnowledge Translation Program, Li Ka Shing Knowledge Institute, St. Michael’s 
33 
Hospital, 209 Victoria Street, East Building, Toronto, Ontario, M5B 1T8, Canada 
34 
bEpidemiology Division, Dalla Lana School of Public Health, University of Toronto, 155 
35 
College Street, 6th floor, Toronto, Ontario, M5T 3M7, Canada 
36 
cDepartment of Physical Therapy, University of Toronto, 160-500 University Ave, 
37 
Toronto, Ontario, M5G 1V7, Canada 
38 
dInstitute of Health Policy, Management and Evaluation (IHPME), University of Toronto, 
39 
155 College Street, 4th Floor, Toronto, Ontario, M5T 3M6, Canada 
40 
eRehabilitation Sciences Institute (RSI), University of Toronto, 500 University Avenue, 
41 
Suite 160, Toronto, Ontario, M5G 1V7, Canada 
42 
fDepartment of Occupational Science & Occupational Therapy, University of Toronto 
43 
160 - 500 University Ave, Toronto, Ontario, M5G 1V7, Canada 
44

3 
 
gDepartment of Physical Therapy, Movement & Rehabilitation Science, Bouvé College 
45 
of Health Sciences, Northeastern University, 360 Huntington Ave, Boston, 
46 
Massachusetts 02115, United States 
47 
hCentre for Journalology, Ottawa Hospital Research Institute, The Ottawa Hospital, 501 
48 
Smyth Road, PO BOX 201B, Ottawa, Ontario, K1H 8L6, Canada 
49 
iJoanna Briggs Institute, The University of Adelaide, Adelaide, South Australia, 5005 
50 
Australia 
51 
jRosemary Bryant AO Research Centre, Sansom Institute for Health Research, 
52 
University of South Australia, Adelaide, South Australia, 5000, Australia 
53 
kThe Royal College of Physicians and Surgeons, 774 Echo Drive, Ottawa, Ontario, K1S 
54 
5N8, Canada 
55 
lCADTH (Canadian Agency for Drugs and Technologies in Health), 865 Carling Ave, 
56 
Suite 600, Ottawa, Ontario, K1S 5S8, Canada 
57 
mRAND Corporation, 1776 Main Street, Santa Monica, California, 90401-3208, United 
58 
States 
59 
nDepartment of Internal Medicine, Faculty of Medicine, Gefinor Center, Block B, 4th 
60 
floor, American University of Beirut, Riad El-Solh, Beirut, Lebanon 
61 
oAgency for Healthcare Research and Quality (AHRQ), 5600 Fishers Lane 
62 
Rockville, MD, 20857, United States 
63 
pDepartment of Medicine, University of Ottawa, Roger Guindon Hall, 451 Smyth Rd, 
64 
Ottawa, Ontario, K1H 8M5, Canada 
65 
qCentre for Reviews and Dissemination, University of York, Heslington, York, YO10 
66 
5DD, United Kingdom 
67

4 
 
rDepartment of Pediatrics, Faculty of Medicine and Dentistry, University of Alberta, 
68 
11405-87 Avenue, Edmonton, Alberta, T6G 1C9, Canada 
69 
sBMJ Open Editorial Office, BMA House, Tavistock Square, London, WC1H 9JR, United 
70 
Kingdom 
71 
tDepartment of Health Research Methods, Evidence, and Impact, McMaster University, 
72 
1280 Main St West, Hamilton, Ontario, L8S 4K1, Canada 
73 
uNorwegian Institute of Public Health, PO Box 4404 Nydalen N-0403, Oslo, Norway 
74 
vHealth Systems Research Unit, South African Medical Research Council, Francie van 
75 
Zyl Drive, Tygerberg, Cape Town, South Africa 
76 
wQueen’s Collaboration for Health Care Quality: A JBI Centre of Excellence, Queen’s 
77 
University School of Nursing, 992 University Avenue, Barrie Street, Kingston, Ontario, 
78 
K7L 3N6, Canada 
79 
xSchool of Nursing, Dalhousie University, PO Box 15000, 5869 University Avenue, 
80 
Halifax, Nova Scotia, B3H 4R2, Canada 
81 
yAlliance for Health Policy and Systems Research, World Health Organization, Avenue 
82 
Appia 20, 1211 Geneva, Switzerland 
83 
zCochrane Editorial Unit, Cochrane, St Albans House, 57-59 Haymarket, London, 
84 
SW1Y 4QX, United Kingdom 
85 
aaSocial Care Workforce Research Unit, King's College London, Strand, London, WC2R 
86 
2LS, United Kingdom 
87 
abUNDP-UNFPA-UNICEF-WHO-World Bank Special Programme of Research, 
88 
Development and Research Training in Human Reproduction (HRP), World Health 
89 
Organization, 20 Avenue Appia, 1211 Geneva, Switzerland 
90

5 
 
acDepartment of Reproductive Health and Research (RHR), World Health Organization, 
91 
Avenue Appia 20, 1211 Geneva, Switzerland 
92 
adDepartment of Geriatric Medicine, University of Toronto, 27 Kings College Circle, 
93 
Toronto, Ontario, M5S 1A1, Canada 
94 
 
95 
*Correspondence and requests for single reprints: 
96 
Dr. Andrea C. Tricco, PhD 
97 
Scientist, Knowledge Translation Program,  
98 
Li Ka Shing Knowledge Institute, St. Michael’s Hospital,  
99 
209 Victoria Street, East Building, Toronto, Ontario, M5B 1W8, Canada 
100 
Phone : 416-864-6060, Fax : 416-864-5805, Email : triccoa@smh.ca 
101 
 
102 
Keywords: knowledge synthesis, scoping reviews, reporting guidelines, research 
103 
methodology  
104 
Running Title: The PRISMA-ScR statement 
105 
Trial registration - EQUATOR registration: http://www.equator-
106 
network.org/library/reporting-guidelines-under-development/#55 
107 
Word Count: 147/200 (Abstract); 2583/3,500 words (Manuscript); 59/75 References; 1 
108 
Figure; 1 Table; 3 Supplements
109

6 
 
ABSTRACT 
110 
Scoping reviews, a type of knowledge synthesis, follow a systematic approach to map 
111 
evidence on a topic; identify main concepts, theories and sources; and determine where 
112 
the gaps are. Though increasing in numbers, the methodological quality and reporting 
113 
quality of scoping reviews need improvement. This document presents the Preferred 
114 
Reporting Items for Systematic Reviews and Meta-Analyses extension for scoping 
115 
reviews (PRISMA-ScR) checklist and explanation.  Developed by a 26-member expert 
116 
panel according to published guidance by the EQUATOR (Enhancing the QUAlity and 
117 
Transparency Of health Research) Network, the checklist contains 20 essential items 
118 
plus 2 optional items. A rationale, along with an example of good reporting, is provided 
119 
for each item. The intent of the PRISMA-ScR is to help readers, including researchers, 
120 
publishers, commissioners, policy-makers, healthcare providers, guideline developers, 
121 
and patients/consumers develop a greater understanding of relevant terminology, core 
122 
concepts and key items to report for scoping reviews. 
123 
 
 
124

7 
 
1. INTRODUCTION 
125 
Scoping reviews can be conducted to meet various objectives.  They may examine the 
126 
extent (i.e., size), range (i.e., variety) and nature (i.e., characteristics) of the evidence on 
127 
a topic or question; determine the value of undertaking a systematic review; summarize 
128 
findings from a body of knowledge that is heterogeneous in terms of methods or 
129 
discipline; or identify gaps in the literature to aid planning and commissioning of future 
130 
research (1, 2). A recent scoping review by members of our team showed that while the 
131 
number of scoping reviews in the literature is increasing steadily, evidence suggests 
132 
that both their methodological quality and reporting quality need to improve to facilitate 
133 
complete and transparent reporting (1). Results from our survey on scoping review 
134 
terminology, definitions and methods revealed a lack of consensus on how to conduct 
135 
and report scoping reviews (3).  
136 
The Joanna Briggs Institute (JBI) published guidance for the conduct of scoping reviews 
137 
in 2015 (4) (which was updated in 2017) (5), based on earlier work by Arksey and 
138 
O’Malley (6) and Levac et al. (7). However, a reporting guideline for scoping reviews 
139 
currently does not exist.  
140 
Reporting guidelines outline a minimum set of items to include in research reports and 
141 
have been shown to increase methodological transparency and uptake of research 
142 
findings (8, 9). Although a reporting guideline exists for systematic reviews, the 
143 
Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 
144 
Statement (10), scoping reviews serve a different purpose than systematic reviews (11). 
145 
Systematic reviews are useful for answering clearly defined questions (such as, Does 
146 
this intervention improve specified outcomes when compared to a given comparator in 
147

8 
 
this population?), whereas scoping reviews are useful for answering much broader 
148 
questions (such as, What is the nature of the evidence for this intervention? Or What is 
149 
known about this concept?). Given the difference in objectives, and therefore, in the  
150 
methodological approach (e.g., presence vs. absence of a risk of bias assessment or 
151 
meta-analysis), the reporting items considered to be essential for systematic reviews 
152 
would differ for scoping reviews –  i.e., some PRISMA items may not be appropriate, 
153 
while other important considerations may be missing (12-14). We deemed that a 
154 
PRISMA extension for scoping reviews is needed to provide reporting guidance for this 
155 
specific type of knowledge synthesis. This extension is also intended to be applicable to 
156 
evidence maps (15, 16), which share similarities with scoping reviews, and involve a 
157 
systematic search of a body of literature to identify knowledge gaps, with a visual 
158 
representation of results (e.g., a figure, graph, etc.).  
159 
 
160 
2. METHODS 
161 
The PRISMA extension for scoping reviews (hereafter, the PRISMA-ScR) was 
162 
developed according to published guidance by the EQUATOR (Enhancing the QUAlity 
163 
and Transparency Of health Research) Network for the development of reporting 
164 
guidelines (9). 
165 
2.1 Protocol, advisory board and expert panel 
166 
Our protocol was drafted by the research team and revised, as necessary, by the 
167 
advisory board prior to being listed as a reporting guideline on the EQUATOR (17) and 
168 
PRISMA (18) websites. The research team included two leads (ACT, SES) and two 
169

9 
 
research coordinators (EL, WZ); all of whom did not participate in the scoring exercises, 
170 
and a 4-member advisory board (KOB, HC, DL, DM) with extensive experience with 
171 
scoping reviews and/or the development of reporting guidelines. We aimed to have a 
172 
representative expert panel in terms of geography and stakeholder type; including 
173 
individuals with experience in the conduct, dissemination, or uptake of scoping reviews.  
174 
2.2 Survey development and round 1 of Delphi  
175 
The initial step to developing the Delphi survey via Qualtrics (an online survey platform) 
176 
(19) involved identifying potential modifications to the original 27-item PRISMA 
177 
checklist. The modifications were based on a research program carried out by members 
178 
of the advisory board to better understand scoping review practices (1, 3, 20) and 
179 
included: a broader research question and literature search strategy, optional risk of 
180 
bias assessment and consultation exercise (whereby relevant stakeholders contribute to 
181 
the work, as described in the Arksey and O’Malley framework (6)), and the inclusion of a 
182 
qualitative analysis. For round 1 of scoring, we prepared a draft of the PRISMA-ScR 
183 
(see Supplement 1) and asked expert panel members to rate the extent to which they 
184 
agreed with the inclusion of the list of items in using a 7-point Likert scale (1=entirely 
185 
disagree, 2=mostly disagree, 3=somewhat disagree, 4=neutral, 5=somewhat agree, 
186 
6=mostly agree, 7=entirely agree). Each survey item included an optional text box 
187 
where comments about the respective item(s) could be provided. The research team 
188 
pilot-tested the survey for content and clarity prior to administering it, and we also sent 
189 
bi-weekly reminders to optimize participation.  
190

10 
 
2.3 Survey analysis  
191 
An 85% consensus rule was selected a priori to signify agreement amongst the expert 
192 
panel, to be conservative. This rule required that at a minimum, 85% of the panel mostly 
193 
or entirely agreed (i.e. corresponding to the scoring values of 6 or 7 on the Likert scale 
194 
used for each of the survey items) with the inclusion of the item in the PRISMA-ScR. If 
195 
less than 85% agreement was observed, we considered the item to be discrepant. This 
196 
standard was used for all three rounds of scoring to inform the final checklist. For ease 
197 
and consistency with how the survey questions were worded, we did not include a 
198 
provision for agreement on exclusion (i.e., 85% scoring values of 1 or 2 on the Likert 
199 
scale). We summarized all of the submitted comments to help explain the scorings and 
200 
identify any issues. For the analysis, the results were stratified by group (i.e., in-person 
201 
meeting vs. online, hereafter e-Delphi participants) given the possibility that discrepant 
202 
items could differ between the arms.  
203 
2.4 In-person arm (round 2 of Delphi)  
204 
We established the Chatham House rule (21) at the beginning of the meeting, whereby 
205 
participants are free to use information that is shared but may not reveal the identity or 
206 
the affiliation of the speaker. Expert panel members were provided the following: their 
207 
individual results, the overall group distribution, median and interquartile range and a 
208 
summary of the JBI methodological guidance (4), as well as preliminary feedback from 
209 
the E-Delphi arm (described below). These data were used to generate and inform the 
210 
discussion about each of the discrepant items from round one. ACT and SES facilitated 
211 
the discussion using a modified nominal group technique (22), a consensus-building 
212

11 
 
method and panel members were subsequently asked to re-score the discrepant items 
213 
using sli.do (23), a live audience-response system in a format that resembled the round 
214 
one survey. For items that failed to meet the threshold for consensus, working groups 
215 
were assembled (described below). The meeting was audio-recorded and transcribed 
216 
using Transcribe Me (24), and 3 note-takers independently documented the main 
217 
discussion points. The transcript was annotated to complement a master summary of 
218 
the discussion points, which was compiled using the 3 note-takers’ files.  
219 
2.5 E-Delphi arm (round 2 of Delphi) 
220 
Those who were unable to attend the in-person meeting participated via an online 
221 
discussion exercise using Conceptboard (25), a visual collaboration platform that allows 
222 
users to provide feedback on ‘whiteboards’ in real-time. We presented the discrepant 
223 
items from round one as a single board in Conceptboard (25) with questions (e.g., “After 
224 
reviewing your survey results with respect to this item, please share why you rated this 
225 
item the way you did”) assigned to participants as tasks, to facilitate the discussion. E-
226 
Delphi panel members were provided with the same materials as those distributed at 
227 
the meeting and were encouraged to respond to others’ comments and interact through 
228 
a chat feature. The second round of scoring was conducted in Qualtrics using a similar 
229 
format as in round one. We shared a summary of the Conceptboard (25) discussion, as 
230 
well as the annotated meeting transcript and master summary document so that 
231 
participants could learn about the perspectives of the in-person group before re-scoring.  
232

12 
 
2.6 Working groups and round 3 of Delphi  
233 
To enable panel-wide dialogue and refine the checklist items prior to the final round of 
234 
scoring, we created working groups that collaborated by teleconference and email. 
235 
Their task was to discuss the discrepant items; in terms of the key issues and 
236 
considerations (relating to both concepts and wording) that had been raised in earlier 
237 
stages, across both arms. To unite the data from the two arms, we conducted a third 
238 
round of scoring using Qualtrics (19). This step involved the full panel scoring an 
239 
updated list of items that had failed to reach consensus in the first two rounds across 
240 
both arms, with the suggested modifications (relating to both concepts and wording) 
241 
from all previous stages incorporated.  
242 
2.7 Interactive workshop (testing) 
243 
A workshop led by ACT and facilitated by members of the advisory board/expert panel 
244 
(SES, CMG, CG, TH, MTM, and MDJP) was held as part of the Global Evidence 
245 
Summit in Cape Town, South Africa in September 2017. The PRISMA-ScR was applied 
246 
to a scoping review on a health-related topic (26) by participants (e.g., researchers, 
247 
scientists, policy makers, managers, and students) to test the checklist .  
248 
3. RESULTS 
249 
3.1 Expert panel 
250 
A total of 37 individuals were invited to participate – of these, 31 people completed 
251 
round 1 and 24 completed all 3 rounds of scoring. Results of the modified Delphi, 
252

13 
 
including the number of items that met agreement at each stage are presented in Figure 
253 
1. 
254 
3.2 Round 1 of Delphi  
255 
For the in-person arm, which involved 16 individuals, 9 of the 27 items reached 
256 
agreement. For the discrepant items, agreement ranged from 56% for item 15 (risk of 
257 
bias) to 81% for items 3 (rationale), 16 (additional analyses), 20 (results of individual 
258 
sources) and 23 (additional analyses). For the E-Delphi arm, which involved 15 
259 
individuals, 8 of the 27 items met the 85% agreement threshold. For the discrepant 
260 
items, agreement ranged from 40% for item 12 (risk of bias) to 80% for items 3 
261 
(rationale), 25 (limitations) and 26 (conclusions).  
262 
3.3 In-person meeting and round 2 of Delphi  
263 
The 16 panel members who attended the in-person meeting in Toronto on November 
264 
29th, 2016 were largely from North America, along with others from Australia, Lebanon, 
265 
and the United Kingdom. Of the 18 discrepant items from round 1, 11 were re-scored 
266 
after discussion. All reached the 85% threshold of agreement, except for one – item 7, 
267 
information sources, which had 83% agreement. For the remaining seven items, the 
268 
group felt that notable changes to the items were required, which formed the basis of 
269 
action by the working groups.  
270 
3.4 E-Delphi online discussion and round 2 Delphi  
271 
Fifteen panel members were invited to participate in the online discussion exercise, 
272 
from countries including Canada, United Kingdom, Switzerland, Norway, and South 
273

14 
 
Africa. Overall, 50% of panelists participated in at least one discussion on 
274 
Conceptboard (25) (7/14) and 1 dropped out. Eleven individuals completed the second 
275 
scoring exercise of the 19 discrepant items, whereby 5 items reached 85% agreement.  
276 
3.5 Working groups and round 3 of Delphi 
277 
There were 6 working groups (with one call per group), ranging in size from three to 
278 
eight participants, with an average of five people per group. For round 3 of the Delphi, 
279 
the 11 items that reached consensus during either round one or round two across both 
280 
the in-person and E-Delphi arms were not included. The survey focused on the 
281 
remaining 16 items that failed to reach consensus across both arms, to ensure that 
282 
decisions made by one arm did not take precedence over the other.  
283 
A total of 27 people were invited to participate in round 3 of the Delphi; 16 from the in-
284 
person meeting arm and 11 from the E-Delphi arm. Overall, 24 out of 27 completed the 
285 
final round of scoring and 3 individuals withdrew (2 from the in-person arm and 1 from 
286 
the E-Delphi). Two of the 16 applicable items failed to meet the 85% agreement 
287 
threshold; items 10 (data collection process) and 15 (risk of bias across studies). Item 
288 
15 was subsequently removed from the checklist, though item 10 was retained but 
289 
revised to exclude the optional consultation exercise step described by Arksey and 
290 
O’Malley and Levac et al., which was the source of the disagreement. Furthermore, it 
291 
was decided that the consultation exercise could be considered a knowledge translation 
292 
activity, which could be conducted for any type of knowledge synthesis.  
293

15 
 
3.6 Interactive workshop (testing) 
294 
A total of 30 participants attended an interactive workshop at the Global Evidence 
295 
Summit in September 2017 in Cape Town, South Africa, where minor revisions were 
296 
suggested for wording of the items.   
297 
3.7 PRISMA-ScR checklist  
298 
The final checklist, with 20 items plus two optional items, is presented in Table 1. It 
299 
consists of 10 items that reached agreement in rounds 1 and 2 (1,3,5,6,8,9,17,25-27), 
300 
along with the 10 items that were agreed upon in round 3 (2,4, 7,10,11,14,18,20,21,24). 
301 
Five items from the original PRISMA were deemed not relevant. They included: items 
302 
13 (summary measures, excluded after round 1) and the following 4 items, which were 
303 
excluded after round 3: 15 (risk of bias across studies), 16 (additional analyses), 22 (risk 
304 
of bias across studies results), and 23 (additional analyses results). See Figure 1 for an 
305 
illustration of the process. In addition, because scoping reviews can include many 
306 
different types of evidence (e.g., documents, blogs, websites, studies, interviews, 
307 
opinions) and are not conducted to examine the risk of bias of the included sources, 
308 
items 12 (risk of bias in individual studies) and 19 (risk of bias within studies results) 
309 
from the original PRISMA are treated as optional in the PRISMA-ScR.  
310 
 
311 
3.8 PRISMA-ScR Explanation and Elaboration  
312 
Each of the PRISMA-ScR checklist items is elaborated upon in Supplement 2.  In this 
313 
document, each item is defined and accompanied by examples of good reporting from 
314

16 
 
existing scoping reviews to provide authors with additional guidance on how to use the 
315 
PRISMA-ScR. 
316 
4. DISCUSSION  
317 
The PRISMA-ScR is intended to provide guidance on the reporting of scoping reviews. 
318 
To develop this PRISMA extension, we adapted the original PRISMA Statement and 
319 
made the following revisions: five items were removed (as they were deemed not 
320 
relevant to scoping reviews), two items were deemed optional, and the wording was 
321 
modified for all of the items. Our reporting guideline is consistent with the JBI guidance 
322 
for scoping reviews, as the JBI guidance is detailed and highlights the importance of 
323 
methodological rigor in the conduct of scoping reviews. We hope that the PRISMA-ScR 
324 
will improve the reporting of scoping reviews and increase their relevance for decision-
325 
making, and that adherence to our reporting guideline will be evaluated in the future, 
326 
which will be critical to measure its impact. 
327 
 
328 
The PRISMA-ScR will be housed on the websites of the EQUATOR Network’s library of 
329 
reporting guidelines and the Knowledge Translation Program of St. Michael’s Hospital 
330 
(27). To promote its uptake, we will create 1-minute YouTube videos to outline how to 
331 
operationalize each of the items; offer webinars for organizations that conduct scoping 
332 
reviews, and create 1-page tip sheets for each item. In the future, we will consider 
333 
creating an automated email PRISMA-ScR dissemination tool, as well as an online tool 
334 
similar to Penelope, which verifies manuscripts for completeness and provides feedback 
335 
to authors as they prepare to submit their work to the BMJ Open journal (28). We will 
336 
share the PRISMA-ScR widely within our networks, including the Alliance for Health 
337

17 
 
Policy and Systems Research, the World Health Organization (WHO) (29) and the 
338 
Global Evidence Synthesis Initiative (30). We will also l collect and review readers’ 
339 
suggestions to improve uptake of the PRISMA-ScR via an online feedback form on the 
340 
Knowledge Translation Program of St. Michael’s Hospital’s website (27). 
341 
 
342 
Study Protocol: Available at EQUATOR and PRISMA websites. 
343 
Data Set: Available from corresponding author.  
344

18 
 
CONTRIBUTIONS 
345 
ACT developed the original idea, oversaw all stages of the project, facilitated the in-
346 
person meeting, wrote the manuscript draft, and is the guarantor for this manuscript. EL 
347 
wrote sections of the manuscript and coordinated and operationalized all stages of the 
348 
project with WZ. KOB, HC, DL, DM, MDJP, TH, LW, SH, EAA, CC, JM, LS, LH, AA, 
349 
MGW, CG, SL, CMG, MTM, EVL, KS, JM, TC, and OT completed round 1 of scoring. 
350 
KOB, HC, DL, MDJP, TH, LW, SH, EAA, CC, JM, LS, LH, AA, and MGW attended the 
351 
in-person meeting and completed round 2 of scoring. CG, SL, CMG, EVL, and KS 
352 
provided feedback on Conceptboard. DM, CG, SL, CMG, MTM, EVL, KS, JM, TC, and 
353 
OT completed the E-Delphi round 2 of scoring. KOB, HC, DL, DM, MDJP, TH, LW, SH, 
354 
EAA, CC, JM, LS, LH, AA, CG, SL, MTM, and KS participated in the working group 
355 
discussions. KOB, HC, DL, DM, MDJP, TH, LW, SH, EAA, CC, JM, LS, LH, AA, MGW, 
356 
CG, SL, CMG, MTM, EVL, KS, JM, TC, and OT completed the final round of scoring. 
357 
SES developed the original idea, oversaw all stages of the project and facilitated the in-
358 
person meeting. All authors critically reviewed the manuscript and approved the final 
359 
version. 
360 
ACKNOWLEDGEMENTS 
361 
We would like to thank the following individuals: 
362 
Susan Le for supporting the coordination of the project and formatting the manuscript. 
363 
Anna Lambrinos for participating in round 1 of scoring and attending the in-person 
364 
meeting. 
365 
Mai Pham for participating in round 1 of scoring and attending the in-person meeting. 
366

19 
 
Lisa O’Malley for participating in round 1 of scoring and in the E-Delphi round 2 of 
367 
scoring.  
368 
Peter Griffiths for participating in round 1 of scoring and providing feedback on 
369 
Conceptboard. 
370 
Charles Shey Wiysonge for participating in round 1 of scoring and providing feedback 
371 
on Conceptboard. 
372 
Jill Manthorpe for participating in round 1 of scoring. 
373 
Mary Ann McColl for participating in round 1 of scoring. 
374 
Assem M Khamis for assisting with the identification of examples for the Explanation 
375 
and Elaboration document. 
376 
Melissa Chen for providing administrative support for the in-person meeting. 
377 
Jessica Comilang for providing administrative support for the in-person meeting. 
378 
Meghan Storey for providing administrative support for the in-person meeting. 
379 
FUNDING 
380 
This work was supported by a Knowledge Synthesis grant from the Canadian Institutes 
381 
of Health Research (CIHR) [grant # KRS 144046]. This funding body had no role in 
382 
designing the study, in collecting, analyzing and interpreting the data, in writing this 
383 
manuscript, and in deciding to submit it for publication. ACT is funded by a Tier 2 
384 
Canada Research Chair in Knowledge Synthesis. KOB was supported by a Canadian 
385 
Institutes of Health Research (CIHR) New Investigator Award. SES is funded by a Tier 1 
386 
Canada Research Chair in Knowledge Translation. 
387

20 
 
COMPETING INTERESTS 
388 
DM led the development of PRISMA, has been involved in the development of several 
389 
PRISMA extensions, is an executive member of the EQUATOR Network, and is the 
390 
director of the Canadian EQUATOR Centre. MDJP is the chair of the Joanna Briggs 
391 
Institute Working Group for Scoping Review Methodology and is the lead author of the 
392 
Joanna Briggs Institute Scoping Review Guidance chapters and articles. CMG is a 
393 
contributing author on the Joanna Briggs Institute manuscript Guidance for conducting 
394 
systematic scoping reviews. KS is a full-time employee of Cochrane. All other authors 
395 
have no potential (or perceived) conflicts of interest to declare. SES is an associate 
396 
editor for the Annals of Internal Medicine; she was not involved in the peer review 
397 
process or decision-making of the manuscript. 
398 
ETHICAL APPROVAL 
399 
Research ethics approval (REB 16-176) for this study was granted by the St. Michael’s 
400 
Hospital Research Ethics Board on August 15th, 2016. 
401 
DATA SHARING 
402 
The results from the three rounds of scoring are available from the corresponding 
403 
author upon reasonable request. 
404 
TRANSPARENCY STATEMENT 
405 
The lead author affirms that the manuscript is an honest, accurate, and transparent 
406 
account of the study being reported; that no important aspects of the study have been 
407

21 
 
omitted; and that any discrepancies from the study as planned (and, if relevant, 
408 
registered) have been explained. 
409 
SUPPLEMENTARY FILES 
410 
Supplement 1: PRISMA-ScR round 1 survey (with information sheet) 
411 
Supplement 2: The PRISMA Extension for Scoping Reviews (PRISMA-ScR): 
412 
Explanation and Elaboration 
413 
Supplement 3: Letters of Permission 
414 
FIGURES 
415 
Figure 1: Methods flow  
416 
TABLES 
417 
Table 1: PRISMA-ScR checklist
418

22 
 
Table 1: PRISMA-ScR Checklist 
419 
 
420 
Section 
Item 
 
PRISMA-ScR checklist item  
Reported 
on page # 
Title 
 
 
 
Title 
1 
Identify the report as a scoping review.  
 
Abstract 
 
 
 
Structured summary 
2 
Provide a structured summary including, as applicable: background, objectives, 
eligibility criteria, sources of evidence, charting methods, results and conclusions that 
relate to the review question(s) and objective(s).  
 
Introduction 
 
 
 
Rationale 
3 
Describe the rationale for the review in the context of what is already known. Explain 
why the review question(s)/objective(s) lend themselves to a scoping review 
approach. 
 
Objectives 
4 
Provide an explicit statement of the question(s) and objective(s) being addressed with 
reference to their key elements (e.g., population or participants, concepts and 
context), or other relevant key elements used to conceptualize the review question(s) 
and/or objective(s)). 
 
Methods 
 
 
 
Protocol and  
registration 
5 
Indicate if a review protocol exists, if and where it can be accessed (e.g., web 
address), and, if available, provide registration information including registration 
number. 
 
Eligibility  
criteria 
6 
Specify the characteristics of the sources of evidence (e.g., years considered, 
language, publication status) used as criteria for eligibility, and provide a rationale. 
 
Information  
sources 
7 
Describe all information sources (e.g., databases with dates of coverage, contact with 
authors to identify additional sources) in the search, as well as the date the most 
recent search was executed.  
 
Search 
8 
Present the full electronic search strategy for at least one database, including any 
limits used, such that it could be repeated. 
 
Selection of sources of 
9 
State the process for selecting sources of evidence (i.e., screening, eligibility) included

23 
 
Section 
Item 
 
PRISMA-ScR checklist item  
Reported 
on page # 
evidence 
in the scoping review. 
Data  
charting 
process 
10 
Describe the methods of charting data from the included sources of evidence (e.g., 
piloted forms; forms that have been tested by the team before their use, whether 
data charting was done independently, in duplicate) and any processes for obtaining 
and confirming data from investigators. 
 
Data items 
11 
List and define all variables for which data were sought and any assumptions and 
simplifications made.  
 
Critical appraisal of 
individual sources of 
evidence 
12 
If done, provide a rationale for conducting a critical appraisal of included sources of 
evidence; describe the methods used and how this information was used in any data 
synthesis (if appropriate). 
 
Summary  
measures 
13 
Not applicable for scoping reviews.  
 
Synthesis of  
results 
14 
Describe the methods of handling and summarizing the data that were charted.   
 
Risk of bias  
across  
studies 
15 
Not applicable for scoping reviews. 
 
Additional analyses 
16 
Not applicable for scoping reviews.  
 
Results 
 
 
 
Selection of sources of 
evidence 
 
17 
Give numbers of sources of evidence screened, assessed for eligibility, and included in 
the review, with reasons for exclusions at each stage, ideally using a flow diagram. 
 
Characteristics of 
sources of evidence 
18 
For each source of evidence, present characteristics for which data were charted and 
provide the citations. 
 
Critical appraisal 
within sources of 
evidence 
19 
If done, present data on critical appraisal of included sources of evidence (see item 
12). 
 
Results of individual 
sources of evidence 
20 
For each included source of evidence, present the relevant data that were charted 
that relate to the review question(s) and objective(s). 
 
Synthesis of  
21 
Summarize and/or present the charting results as they relate to the review

24 
 
Section 
Item 
 
PRISMA-ScR checklist item  
Reported 
on page # 
results 
question(s) and objective(s). 
Risk of bias  
across  
studies 
22 
Not applicable for scoping reviews. 
 
Additional analyses 
23 
Not applicable for scoping reviews. 
 
Discussion 
 
 
 
Summary of  
evidence 
24 
Summarize the main results (including an overview of concepts, themes, and types of 
evidence available), explain how they relate to the review question(s) and objectives, 
and consider the relevance to key groups. 
 
Limitations 
25 
Discuss the limitations of the scoping review process. 
 
Conclusions 
26 
Provide a general interpretation of the results with respect to the review question(s) 
and objective(s), as well as potential implications and/or next steps. 
 
Funding 
 
 
 
Funding 
27 
Describe sources of funding for the included sources of evidence, as well as sources of 
funding for the scoping review. Describe the role of the funders of the scoping review. 
 
 
421 
Mini-glossary of PRISMA-ScR terms 
Charting – The process of data extraction in a scoping review is referred to as ‘data charting’, as per the Arksey and O’Malley (2005) and Levac et 
al. (2010) frameworks and the JBI guidance (2015, 2017). 
Critical appraisal – Refers to the process of systematically examining research evidence to assess its validity, results and relevance before using it 
to inform a decision. This terminology is used for items 12 and 19, instead of ‘risk of bias’ (which is more applicable to systematic reviews of 
interventions) to be inclusive and acknowledge the various sources of evidence that may be included in a scoping review (e.g., quantitative 
and/or qualitative research, expert opinion, policy documents). 
Information sources - This is where sources of evidence (see definition) are compiled from such as, bibliographic databases, social media 
platforms, websites, etc.  
Sources of evidence – A more inclusive/ heterogeneous term is used to account for the fact that different types of evidence or data sources (e.g., 
quantitative and/or qualitative research, expert opinion, policy documents) may be eligible in a scoping review, as opposed to only studies.  This 
is not to be confused with information sources (see definition).

25 
 
REFERENCES 
422 
1. 
Tricco AC, Lillie E, Zarin W, O'Brien K, Colquhoun H, Kastner M, et al. A scoping 
423 
review on the conduct and reporting of scoping reviews. BMC Med Res Methodol. 
424 
2016;16:15. 
425 
2. 
A Guide to Knowledge Synthesis: A Knowledge Synthesis Chapter: Canadian 
426 
Institutes of Health Research; 2010. Available from: http://www.cihr-
427 
irsc.gc.ca/e/41382.html. Accessed on 10 January 2018. 
428 
3. 
Colquhoun HL, Levac D, O'Brien KK, Straus S, Tricco AC, Perrier L, et al. 
429 
Scoping reviews: time for clarity in definition, methods, and reporting. J Clin Epidemiol. 
430 
2014;67(12):1291-4. 
431 
4. 
Peters MD, Godfrey CM, Khalil H, McInerney P, Parker D, Soares CB. Guidance 
432 
for conducting systematic scoping reviews. Int J Evid Based Healthc. 2015;13(3):141-6. 
433 
5. 
Peters MDJ, Godfrey C, McInerney P, Baldini Soares C, Khalil H, Parker D. 
434 
Chapter 11: Scoping Reviews. In: Aromataris E, Munn Z (Editors). Joanna Briggs 
435 
Institute Reviewer's Manual. The Joanna Briggs Institute, 2017. Available from 
436 
https://reviewersmanual.joannabriggs.org/. Accessed on 14 June 2018. 
437 
6. 
Arksey H, O'Malley L. Scoping studies: towards a methodological framework. 
438 
International Journal of Social Research Methodology. 2005;8(1):19-32. 
439 
7. 
Levac D, Colquhoun H, O'Brien KK. Scoping studies: advancing the 
440 
methodology. Implementation science : IS. 2010;5:69. 
441 
8. 
Altman DG, Simera I. Using Reporting Guidelines Effectively to Ensure Good 
442 
Reporting of Health Research.  Guidelines for Reporting Health Research: A User's 
443 
Manual 2014. p. 32-40. 
444 
9. 
Moher D, Schulz KF, Simera I, Altman DG. Guidance for developers of health 
445 
research reporting guidelines. PLoS Med. 2010;7(2):e1000217. 
446 
10. 
Moher D, Liberati A, Tetzlaff J, Altman DG. Preferred reporting items for 
447 
systematic reviews and meta-analyses: the PRISMA statement. Bmj. 2009;339:b2535. 
448 
11. 
Tricco AC, Zarin W, Ghassemi M, Nincic V, Lillie E, Page MJ, et al. Same family, 
449 
different species: methodological conduct and quality varies according to purpose for 
450 
five types of knowledge synthesis. J Clin Epidemiol. 2017. 
451 
12. 
McInnes MD, Bossuyt PM. Pitfalls of Systematic Reviews and Meta-Analyses in 
452 
Imaging Research. Radiology. 2015;277(1):13-21. 
453 
13. 
Macaskill P, Gatsonis C, Deeks JJ, Harbord RM, Takwoingi Y. Chapter 10: 
454 
Analysing and Presenting Results. In: Deeks JJ, Bossuyt PM, Gatsonis C (editors), 
455 
Cochrane Handbook for Systematic Reviews of Diagnostic Test Accuracy Version 1.0. 
456 
The Cochrane Collaboration, 2010. Available from: http://srdta.cochrane.org/. Accessed 
457 
on 14 June 2018. 
458 
14. 
Whiting PF, Rutjes AW, Westwood ME, Mallett S, Deeks JJ, Reitsma JB, et al. 
459 
QUADAS-2: a revised tool for the quality assessment of diagnostic accuracy studies. 
460 
Ann Intern Med. 2011;155(8):529-36. 
461 
15. 
Schmucker C, Motschall E, Antes G, Meerpohl JJ. [Methods of evidence 
462 
mapping. A systematic review]. Bundesgesundheitsblatt, Gesundheitsforschung, 
463 
Gesundheitsschutz. 2013;56(10):1390-7. 
464

26 
 
16. 
Miake-Lye IM, Hempel S, Shanman R, Shekelle PG. What is an evidence map? 
465 
A systematic review of published evidence maps and their definitions, methods, and 
466 
products. Systematic Reviews. 2016;5(1):28. 
467 
17. 
Reporting guidelines under development: Preferred Reporting Items for 
468 
Systematic Reviews and Meta-Analysis extension for Scoping Reviews (PRISMA-ScR): 
469 
The EQUATOR Network; 2017. Available from: http://www.equator-
470 
network.org/library/reporting-guidelines-under-development/#55. Accessed on 10 
471 
January 2018. 
472 
18. 
Extensions in Development: Preferred Reporting Items for Systematic Reviews 
473 
and Meta-Analyses (PRISMA). Available from: http://www.prisma-
474 
statement.org/Extensions/InDevelopment.aspx. Accessed on 10 January 2018. 
475 
19. 
Qualtrics 2018. Available from: https://www.qualtrics.com/uk/. Accessed on 10 
476 
January 2018. 
477 
20. 
O'Brien KK, Colquhoun H, Levac D, Baxter L, Tricco AC, Straus S, et al. 
478 
Advancing scoping study methodology: a web-based survey and consultation of 
479 
perceptions on terminology, definition and methodological steps. BMC health services 
480 
research. 2016;16:305. 
481 
21. 
The Royal Institute of International Affairs. Chatham House Rule, 2018. Available 
482 
from https://www.chathamhouse.org/chatham-house-rule. Accessed on 14 June 2018. 
483 
22. 
Jones J, Hunter D. Consensus methods for medical and health services 
484 
research. Bmj. 1995;311(7001):376-80. 
485 
23. 
Sli.do 2012. Available from: https://www.sli.do/. Accessed on 10 January 2018. 
486 
24. 
TranscribeMe 2018. Available from: https://transcribeme.com/. Accessed on 27 
487 
February 2018. 
488 
25. 
Conceptboard 2018. Available from: https://conceptboard.com/. Accessed on 14 
489 
June 2018. 
490 
26. 
Lourida I, Abbott RA, Rogers M, Lang IA, Stein K, Kent B, et al. Dissemination 
491 
and implementation research in dementia care: a systematic scoping review and 
492 
evidence map. BMC Geriatr. 2017;17(1):147. 
493 
27. 
Knowledge Translation Program 2016. Available from: 
494 
https://knowledgetranslation.net/. Accessed on 10 January 2018. 
495 
28. 
Harwood J. Penelope London, UK Squarespace; 2017. Available from: 
496 
https://www.penelope.ai/. Accessed on 28 February 2018. 
497 
29. 
The Alliance for Health Policy and Systems Research 2018. Available from: 
498 
http://www.who.int/alliance-hpsr/en/. Accessed on 27 February 2018. 
499 
30. 
Global Evidence Synthesis Initiative (GESI) 2016. Available from: 
500 
http://www.gesiinitiative.com/. Accessed on 27 February 2018. 
501 
31. 
San A, Hiremagalur B, Muircroft W, Grealish L. Screening of Cognitive 
502 
Impairment in the Dialysis Population: A Scoping Review. Dement Geriatr Cogn Disord. 
503 
2017;44(3-4):182-95. 
504 
32. 
Galloway T, Blackett H, Chatwood S, Jeppesen C, Kandola K, Linton J, et al. 
505 
Obesity studies in the circumpolar Inuit: a scoping review. Int J Circumpolar Health. 
506 
2012;71:18698. 
507 
33. 
Beller EM, Glasziou PP, Altman DG, Hopewell S, Bastian H, Chalmers I, et al. 
508 
PRISMA for Abstracts: reporting systematic reviews in journal and conference 
509 
abstracts. PLoS Med. 2013;10(4):e1001419. 
510

27 
 
34. 
Hopewell S, Clarke M, Moher D, Wager E, Middleton P, Altman DG, et al. 
511 
CONSORT for reporting randomized controlled trials in journal and conference 
512 
abstracts: explanation and elaboration. PLoS Med. 2008;5(1):e20. 
513 
35. 
Haynes RB, Mulrow CD, Huth EJ, Altman DG, Gardner MJ. More informative 
514 
abstracts revisited. Ann Intern Med. 1990;113(1):69-76. 
515 
36. 
Piskur B, Beurskens AJ, Jongmans MJ, Ketelaar M, Norton M, Frings CA, et al. 
516 
Parents' actions, challenges, and needs while enabling participation of children with a 
517 
physical disability: a scoping review. BMC Pediatr. 2012;12:177. 
518 
37. 
Richardson WS, Wilson MC, Nishikawa J, Hayward RS. The well-built clinical 
519 
question: a key to evidence-based decisions. ACP J Club. 1995;123(3):A12-3. 
520 
38. 
Andrew B. Clear and present questions: formulating questions for evidence 
521 
based practice. Library Hi Tech. 2006;24(3):355-68. 
522 
39. 
The Joanna Briggs Institute. The Joanna Briggs Institute Reviewers’ Manual 
523 
2015: Methodology for JBI Scoping Reviews Adelaide, South Australia: The Joanna 
524 
Briggs Institute; 2015. Available from: 
525 
https://joannabriggs.org/assets/docs/sumari/Reviewers-Manual_Methodology-for-JBI-
526 
Scoping-Reviews_2015_v2.pdf. Accessed on 10 January 2018. 
527 
40. 
Tricco AC, Zarin W, Lillie E, Pham B, Straus SE. Utility of social media and 
528 
crowd-sourced data for pharmacovigilance: a scoping review protocol. BMJ Open. 
529 
2017;7(1):e013474. 
530 
41. 
Open Science Framework 2011. Available from: https://osf.io/. Accessed on 10 
531 
January 2018. 
532 
42. 
Systematic Reviews. Available from: 
533 
https://systematicreviewsjournal.biomedcentral.com/. Accessed on 10 January 2018. 
534 
43. 
JBI Database of Systematic Reviews and Implementation Reports. Available 
535 
from: http://journals.lww.com/jbisrir/pages/default.aspx. Accessed on 10 January 2018. 
536 
44. 
BMJ Open. Available from: http://bmjopen.bmj.com/. Accessed on 01 March 
537 
2018. 
538 
45. 
Sav A, Salehi A, Mair FS, McMillan SS. Measuring the burden of treatment for 
539 
chronic disease: implications of a scoping review of the literature. BMC Med Res 
540 
Methodol. 2017;17(1):140. 
541 
46. 
Cardoso R, Zarin W, Nincic V, Barber SL, Gulmezoglu AM, Wilson C, et al. 
542 
Evaluative reports on medical malpractice policies in obstetrics: a rapid scoping review. 
543 
Syst Rev. 2017;6(1):181. 
544 
47. 
McGowan J, Sampson M, Salzwedel DM, Cogo E, Foerster V, Lefebvre C. 
545 
PRESS Peer Review of Electronic Search Strategies: 2015 Guideline Statement. J Clin 
546 
Epidemiol. 2016;75:40-6. 
547 
48. 
Grey Matters: a practical tool for searching health-related grey literature: 
548 
Canadian Agency for Drugs and Technologies in Health (CADTH); 2015. Available 
549 
from: https://cadth.ca/resources/finding-evidence/grey-matters. Accessed on 10 January 
550 
2018. 
551 
49. 
Duffett M, Choong K, Hartling L, Menon K, Thabane L, Cook DJ. Randomized 
552 
controlled trials in pediatric critical care: a scoping review. Crit Care. 2013;17(5):R256. 
553 
50. 
Lenzen SA, Daniels R, van Bokhoven MA, van der Weijden T, Beurskens A. 
554 
Disentangling self-management goal setting and action planning: A scoping review. 
555 
PloS one. 2017;12(11):e0188822. 
556

28 
 
51. 
Leung M, Perumal N, Mesfin E, Krishna A, Yang S, Johnson W, et al. Metrics of 
557 
early childhood growth in recent epidemiological research: A scoping review. PloS one. 
558 
2018;13(3):e0194565. 
559 
52. 
Tricco AC, Zarin W, Rios P, Nincic V, Khan PA, Ghassemi M, et al. Engaging 
560 
policy-makers, heath system managers, and policy analysts in the knowledge synthesis 
561 
process: a scoping review. Implementation science : IS. 2018;13(1):31. 
562 
53. 
Zarin W, Veroniki AA, Nincic V, Vafaei A, Reynen E, Motiwala SS, et al. 
563 
Characteristics and knowledge synthesis approach for 456 network meta-analyses: a 
564 
scoping review. BMC Med. 2017;15(1):3. 
565 
54. 
Hutchinson J, Prady SL, Smith MA, White PC, Graham HM. A Scoping Review of 
566 
Observational Studies Examining Relationships between Environmental Behaviors and 
567 
Health Behaviors. International journal of environmental research and public health. 
568 
2015;12(5):4833-58. 
569 
55. 
Hosking J, Campbell-Lendrum D. How well does climate change and human 
570 
health research match the demands of policymakers? A scoping review. Environ Health 
571 
Perspect. 2012;120(8):1076-82. 
572 
56. 
Strand M, Gammon D, Ruland CM. Transitions from biomedical to recovery-
573 
oriented practices in mental health: a scoping review to explore the role of Internet-
574 
based interventions. BMC health services research. 2017;17(1):257. 
575 
57. 
Constand MK, MacDermid JC, Dal Bello-Haas V, Law M. Scoping review of 
576 
patient-centered care approaches in healthcare. BMC health services research. 
577 
2014;14:271. 
578 
58. 
Tricco AC, Antony J, Zarin W, Strifler L, Ghassemi M, Ivory J, et al. A scoping 
579 
review of rapid review methods. BMC Med. 2015;13:224. 
580 
59. 
Hall AJ, Lang IA, Endacott R, Hall A, Goodwin VA. Physiotherapy interventions 
581 
for people with dementia and a hip fracture-a scoping review of the literature. 
582 
Physiotherapy. 2017;103(4):361-8. 
583